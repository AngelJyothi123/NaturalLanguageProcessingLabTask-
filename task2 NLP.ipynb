{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNiFe68Q5vQJkrsd5E1yyXZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deFUSQ4U9wyS","executionInfo":{"status":"ok","timestamp":1753942377600,"user_tz":-330,"elapsed":387,"user":{"displayName":"Angeljyothi Cherukuri","userId":"10915435747811708744"}},"outputId":"f71de33c-49c9-4a01-efbd-e32bc8b8d000"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Tokens: ['Tokenization', 'without', 'transformers', 'is', 'straightforward', 'with', 'tools', 'like', 'NLTK', '.']\n","Transformers tokens: {'input_ids': tensor([[  101, 19204,  3989,  2302, 19081,  2003, 19647,  2007,  5906,  2066,\n","         17953,  2102,  2243,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","Transformers Tokens: ['[CLS]', 'token', '##ization', 'without', 'transformers', 'is', 'straightforward', 'with', 'tools', 'like', 'nl', '##t', '##k', '.', '[SEP]']\n","Decoded Text: tokenization without transformers is straightforward with tools like nltk.\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt_tab')\n","text = \"Tokenization without transformers is straightforward with tools like NLTK.\"\n","tokens = word_tokenize(text)\n","print(\"Tokens:\",tokens)\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","text = \"Tokenization without transformers is straightforward with tools like NLTK.\"\n","tokens_transformers = tokenizer(text, return_tensors=\"pt\")\n","print(\"Transformers tokens:\",tokens_transformers)\n","tokens_transformers_list=tokenizer.convert_ids_to_tokens(tokens_transformers['input_ids'][0].numpy().tolist())\n","print(\"Transformers Tokens:\",tokens_transformers_list)\n","decoded_text=tokenizer.decode(tokens_transformers['input_ids'][0],skip_special_tokens=True)\n","print(\"Decoded Text:\",decoded_text)"]}]}