{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9ekT8hNysTZ25gmsNOvgN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xd2qexHDCuR","executionInfo":{"status":"ok","timestamp":1755753877822,"user_tz":-330,"elapsed":64472,"user":{"displayName":"Angeljyothi Cherukuri","userId":"10915435747811708744"}},"outputId":"6d5b645b-0146-4668-ae9a-0505b969e18c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 424ms/step - accuracy: 0.0000e+00 - loss: 4.4679\n","Epoch 2/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 407ms/step - accuracy: 0.1058 - loss: 4.4560\n","Epoch 3/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 889ms/step - accuracy: 0.0618 - loss: 4.4385\n","Epoch 4/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 417ms/step - accuracy: 0.0827 - loss: 4.3772\n","Epoch 5/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 429ms/step - accuracy: 0.0451 - loss: 4.2927\n","Epoch 6/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 435ms/step - accuracy: 0.0555 - loss: 4.1647\n","Epoch 7/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 431ms/step - accuracy: 0.0619 - loss: 4.1034\n","Epoch 8/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 713ms/step - accuracy: 0.0702 - loss: 3.9931\n","Epoch 9/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 450ms/step - accuracy: 0.1226 - loss: 3.9362\n","Epoch 10/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 441ms/step - accuracy: 0.0871 - loss: 3.8187\n","Epoch 11/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 423ms/step - accuracy: 0.0651 - loss: 3.7377\n","Epoch 12/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 436ms/step - accuracy: 0.0610 - loss: 3.7137\n","Epoch 13/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 857ms/step - accuracy: 0.1068 - loss: 3.5280\n","Epoch 14/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 424ms/step - accuracy: 0.0683 - loss: 3.5061\n","Epoch 15/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 438ms/step - accuracy: 0.0965 - loss: 3.5013\n","Epoch 16/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 427ms/step - accuracy: 0.0849 - loss: 3.4394\n","Epoch 17/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 434ms/step - accuracy: 0.0882 - loss: 3.3117\n","Epoch 18/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 849ms/step - accuracy: 0.1006 - loss: 3.2453\n","Epoch 19/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 430ms/step - accuracy: 0.0914 - loss: 3.2055\n","Epoch 20/20\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 426ms/step - accuracy: 0.1269 - loss: 3.1113\n"]}],"source":["import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import spacy\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Downloads\n","nltk.download('punkt')\n","nltk.download('punkt_tab')   # ✅ Fix for error\n","spacy.cli.download(\"en_core_web_sm\")\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Corpus\n","corpus = \"\"\"One disadvantage of using 'Best Of' sampling is that it may lead to limited exploration of the model's\n","knowledge and creativity. By focusing on the most probable next words, the model might generate responses that are\n","safe and conventional, potentially missing out on more diverse and innovative outputs. The lack of exploration could\n","result in repetitive or less imaginative responses, especially in situations where novel and unconventional ideas are\n","desired. To address this limitation, other sampling strategies like temperature-based sampling or top-p (nucleus) sampling\n","can be employed to introduce more randomness and encourage the model to explore a broader range of possibilities.\n","However, it's essential to carefully balance exploration and exploitation based on the specific requirements of the task or\n","application.\"\"\"\n","\n","# Tokenization & Lemmatization\n","tokens = word_tokenize(corpus)\n","lemmatized_tokens = [token.lemma_ for token in nlp(corpus)]\n","\n","# ✅ Use only lemmas (cleaner representation)\n","processed_text = \" \".join(lemmatized_tokens)\n","\n","# Tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([processed_text])\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Input sequences\n","input_sequences = []\n","token_list = tokenizer.texts_to_sequences([processed_text])[0]\n","\n","for i in range(1, len(token_list)):\n","    n_gram_sequence = token_list[:i+1]\n","    input_sequences.append(n_gram_sequence)\n","\n","# Padding\n","max_sequence_length = max(len(seq) for seq in input_sequences)\n","input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n","\n","# Features and labels\n","X, y = input_sequences[:, :-1], input_sequences[:, -1]\n","y = np.array(y)\n","\n","# Model (Improved)\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n","model.add(LSTM(150, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Training\n","history = model.fit(X, y, epochs=20, verbose=1)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"qw8WoHSbD1WW"},"execution_count":null,"outputs":[]}]}