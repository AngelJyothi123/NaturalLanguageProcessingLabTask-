{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1e6oRV7ZUQNzI_bcC24Z95AN-YN5Cfi6P","authorship_tag":"ABX9TyOI+N0KNtIxsrBtt6Eoy8R5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JUxwXOfD9Kt","executionInfo":{"status":"ok","timestamp":1755754363027,"user_tz":-330,"elapsed":402,"user":{"displayName":"Angeljyothi Cherukuri","userId":"10915435747811708744"}},"outputId":"1e7f072b-3dab-45a0-ccf0-f147fb5a6f83"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Morphology of the document (Top 15 words):\n","language: 2\n","artificial: 1\n","intelligence: 1\n","transforming: 1\n","industries: 1\n","worldwide: 1\n","natural: 1\n","processing: 1\n","allows: 1\n","machines: 1\n","understand: 1\n","process: 1\n","human: 1\n","effectively: 1\n","data: 1\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","\n","# Downloads (Fixes LookupError)\n","nltk.download('punkt')\n","nltk.download('punkt_tab')   # ✅ required in latest NLTK\n","nltk.download('stopwords')\n","\n","# Load document\n","def load_document(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        return file.read()\n","\n","# Tokenization\n","def tokenize_document(document):\n","    tokens = word_tokenize(document)\n","    return [word.lower() for word in tokens if word.isalpha()]  # Remove punctuation and convert to lowercase\n","\n","# Remove stopwords\n","def remove_stopwords(tokens):\n","    stop_words = set(stopwords.words('english'))\n","    return [word for word in tokens if word not in stop_words]\n","\n","# Morphology (word frequency distribution)\n","def find_morphology(tokens, top_n=10):\n","    fdist = FreqDist(tokens)\n","    return fdist.most_common(top_n)  # ✅ return only top N most common words\n","\n","# Main execution\n","document_path = \"/content/drive/MyDrive/NLP LAB TASK/NLP Task 4 text.txt\"  # ✅ corrected path\n","document = load_document(document_path)\n","tokens = tokenize_document(document)\n","tokens_without_stopwords = remove_stopwords(tokens)\n","morphology = find_morphology(tokens_without_stopwords, top_n=15)  # ✅ top 15 common words\n","\n","# Print results\n","print(\"Morphology of the document (Top 15 words):\")\n","for word, frequency in morphology:\n","    print(f\"{word}: {frequency}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"rf6RnK_FHDCX"},"execution_count":null,"outputs":[]}]}