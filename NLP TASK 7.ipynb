{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1U5M0iOzGVdgYLBKHfCvNr91J3LKIPqQK","authorship_tag":"ABX9TyN0FzVpVPgv8KaGmY9Kh1lR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**TASK 8.1**"],"metadata":{"id":"U74iNGPbCHxa"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHzLC2gW1ISU","executionInfo":{"status":"ok","timestamp":1757561919268,"user_tz":-330,"elapsed":254987,"user":{"displayName":"Angeljyothi Cherukuri","userId":"10915435747811708744"}},"outputId":"33b78e9d-837b-4e61-819f-37fe82368fa5"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","/tmp/ipython-input-1020540727.py:29: DeprecationWarning: \n","  Function evaluate() has been deprecated.  Use accuracy(gold)\n","  instead.\n","  hmm_accuracy = hmm_tagger.evaluate(test_sents)\n","/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n","/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n","  O[i, k] = self._output_logprob(si, self._symbols[k])\n"]},{"output_type":"stream","name":"stdout","text":["HMM (Viterbi) Accuracy: 0.4488950276243094\n","  ==> Training (10 iterations)\n","\n","      Iteration    Log Likelihood    Accuracy\n","      ---------------------------------------\n","             1          -2.48491        0.065\n","             2          -0.98007        0.815\n","             3          -0.67044        0.921\n","             4          -0.51957        0.948\n","             5          -0.43023        0.960\n","             6          -0.37041        0.967\n","             7          -0.32708        0.971\n","             8          -0.29396        0.975\n","             9          -0.26766        0.978\n","         Final          -0.24619        0.980\n","Log-Linear (MaxEnt) Accuracy: 0.9408050513022889\n","Log-Linear Model performs better.\n"]}],"source":["import nltk\n","from nltk.corpus import treebank\n","from nltk.tag import hmm\n","from nltk.classify import MaxentClassifier\n","from nltk.classify.util import accuracy as nltk_accuracy\n","import random\n","import spacy\n","\n","# Download necessary datasets\n","nltk.download('treebank')\n","nltk.download('universal_tagset')\n","\n","# -------------------------------\n","# Step 1: Load Data\n","# -------------------------------\n","tagged_sentences = list(treebank.tagged_sents(tagset='universal'))\n","random.shuffle(tagged_sentences)\n","\n","train_size = int(0.8 * len(tagged_sentences))\n","train_sents = tagged_sentences[:train_size]\n","test_sents = tagged_sentences[train_size:]\n","\n","# -------------------------------\n","# Step 2: HMM + Viterbi\n","# -------------------------------\n","trainer = hmm.HiddenMarkovModelTrainer()\n","hmm_tagger = trainer.train_supervised(train_sents)\n","\n","hmm_accuracy = hmm_tagger.evaluate(test_sents)\n","print(\"HMM (Viterbi) Accuracy:\", hmm_accuracy)\n","\n","# -------------------------------\n","# Step 3: Log-Linear Model (MaxEnt)\n","# -------------------------------\n","\n","# Feature extractor\n","def extract_features(sentence, index):\n","    word = sentence[index][0]\n","    return {\n","        'word': word,\n","        'suffix(2)': word[-2:],\n","        'prefix(1)': word[0],\n","        'is_first': index == 0,\n","        'is_last': index == len(sentence) - 1,\n","        'prev_word': '' if index == 0 else sentence[index - 1][0],\n","        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0]\n","    }\n","\n","# Prepare training data\n","train_data = []\n","for sent in train_sents:\n","    for i in range(len(sent)):\n","        features = extract_features(sent, i)\n","        label = sent[i][1]\n","        train_data.append((features, label))\n","\n","test_data = []\n","for sent in test_sents:\n","    for i in range(len(sent)):\n","        features = extract_features(sent, i)\n","        label = sent[i][1]\n","        test_data.append((features, label))\n","\n","# Train MaxEnt classifier (log-linear model)\n","maxent_classifier = MaxentClassifier.train(train_data, max_iter=10)\n","\n","# Evaluate\n","maxent_accuracy = nltk_accuracy(maxent_classifier, test_data)\n","print(\"Log-Linear (MaxEnt) Accuracy:\", maxent_accuracy)\n","\n","# -------------------------------\n","# Step 4: Comparison\n","# -------------------------------\n","if maxent_accuracy > hmm_accuracy:\n","    print(\"Log-Linear Model performs better.\")\n","else:\n","    print(\"HMM with Viterbi performs better.\")\n"]},{"cell_type":"markdown","source":["**TAsk 8.2**"],"metadata":{"id":"bZ_4V6kLCM35"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import treebank\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import random\n","\n","# Download required data\n","nltk.download('treebank')\n","nltk.download('universal_tagset')\n","\n","# Load dataset\n","sentences = list(treebank.tagged_sents(tagset='universal'))\n","random.shuffle(sentences)\n","\n","# Train-Test Split\n","train_data = sentences[:3000]\n","test_data = sentences[3000:]\n","\n","######################################\n","# Part 1: HMM with Viterbi Decoding\n","######################################\n","from nltk.tag import hmm\n","\n","trainer = hmm.HiddenMarkovModelTrainer()\n","hmm_tagger = trainer.train_supervised(train_data)\n","\n","hmm_accuracy = hmm_tagger.evaluate(test_data)\n","print(\"HMM (Viterbi) Accuracy:\", hmm_accuracy)\n","\n","######################################\n","# Part 2: Log-Linear Model (MaxEnt)\n","######################################\n","\n","# Feature extractor\n","def extract_features(sentence, i):\n","    word = sentence[i]\n","    features = {\n","        'word': word,\n","        'is_capitalized': word[0].isupper(),\n","        'is_digit': word.isdigit(),\n","        'prefix-1': word[0],\n","        'suffix-1': word[-1],\n","        'suffix-2': word[-2:],\n","    }\n","    if i > 0:\n","        features['prev_word'] = sentence[i-1]\n","    else:\n","        features['prev_word'] = '<START>'\n","    return features\n","\n","# Prepare train and test sets\n","def prepare_dataset(tagged_sents):\n","    X, y = [], []\n","    for sent in tagged_sents:\n","        words, tags = zip(*sent)\n","        for i in range(len(words)):\n","            feats = extract_features(words, i)\n","            X.append(feats)\n","            y.append(tags[i])\n","    return X, y\n","\n","X_train, y_train = prepare_dataset(train_data)\n","X_test, y_test = prepare_dataset(test_data)\n","\n","# Convert features to dict vector\n","from sklearn.feature_extraction import DictVectorizer\n","vec = DictVectorizer(sparse=True)\n","\n","X_train_vec = vec.fit_transform(X_train)\n","X_test_vec = vec.transform(X_test)\n","\n","# Train log-linear model (MaxEnt = Logistic Regression)\n","clf = LogisticRegression(max_iter=200)\n","clf.fit(X_train_vec, y_train)\n","\n","y_pred = clf.predict(X_test_vec)\n","\n","log_linear_accuracy = accuracy_score(y_test, y_pred)\n","print(\"Log-Linear Model Accuracy:\", log_linear_accuracy)\n","\n","######################################\n","# Part 3: Comparison\n","######################################\n","print(\"\\nPerformance Comparison:\")\n","print(f\"HMM (Viterbi): {hmm_accuracy:.4f}\")\n","print(f\"Log-Linear Model: {log_linear_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtpaMJ-x2Pri","executionInfo":{"status":"ok","timestamp":1757561937724,"user_tz":-330,"elapsed":18470,"user":{"displayName":"Angeljyothi Cherukuri","userId":"10915435747811708744"}},"outputId":"afe5c613-e338-4d30-cc55-f9e2eb0777e5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n","/tmp/ipython-input-4039659684.py:27: DeprecationWarning: \n","  Function evaluate() has been deprecated.  Use accuracy(gold)\n","  instead.\n","  hmm_accuracy = hmm_tagger.evaluate(test_data)\n"]},{"output_type":"stream","name":"stdout","text":["HMM (Viterbi) Accuracy: 0.4535505534137545\n","Log-Linear Model Accuracy: 0.9578825187580318\n","\n","Performance Comparison:\n","HMM (Viterbi): 0.4536\n","Log-Linear Model: 0.9579\n"]}]},{"cell_type":"markdown","source":["**Task 9**"],"metadata":{"id":"cJuj3zUCCW7N"}},{"cell_type":"code","source":["import nltk\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","# Download dataset\n","nltk.download('treebank')\n","nltk.download('universal_tagset')\n","nltk.download('punkt')\n","\n","sentences = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n","\n","# Prepare words and tags\n","words = list(set(w for s in sentences for (w, t) in s))\n","tags = list(set(t for s in sentences for (w, t) in s))\n","word2idx = {w: i + 2 for i, w in enumerate(words)}\n","word2idx[\"PAD\"] = 0\n","word2idx[\"UNK\"] = 1\n","tag2idx = {t: i for i, t in enumerate(tags)}\n","\n","X = [[word2idx.get(w, 1) for (w, t) in s] for s in sentences]\n","y = [[tag2idx[t] for (w, t) in s] for s in sentences]\n","\n","max_len = 50\n","X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n","y = pad_sequences(y, maxlen=max_len, padding=\"post\")\n","y = [to_categorical(i, num_classes=len(tags)) for i in y]\n","\n","# Small BiLSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=len(word2idx), output_dim=64, input_length=max_len, mask_zero=True))\n","model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n","model.add(TimeDistributed(Dense(len(tags), activation=\"softmax\")))\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","model.fit(X, np.array(y), batch_size=32, epochs=1, verbose=1)  # small training\n","\n","# ---- Read text from file ----\n","file_path = \"/content/drive/MyDrive/NLP LAB TASK/NLP TASK9.txt\"\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    text = f.read()\n","\n","# Download extra tokenizer resource\n","nltk.download(\"punkt_tab\")\n","\n","# Tokenize\n","tokens = nltk.word_tokenize(text)\n","test_seq = [word2idx.get(w, 1) for w in tokens]\n","test_seq = pad_sequences([test_seq], maxlen=max_len, padding=\"post\")\n","\n","# Predict tags\n","pred = model.predict(test_seq)[0]\n","pred_tags = [list(tag2idx.keys())[np.argmax(p)] for p in pred][:len(tokens)]\n","\n","print(\"\\nTagged Output:\")\n","for word, tag in zip(tokens, pred_tags):\n","    print(f\"{word} --> {tag}\")\n","\n","# Extract nouns, proper nouns, numbers\n","extracted_info = [word for word, tag in zip(tokens, pred_tags) if tag in [\"NOUN\", \"PROPN\", \"NUM\"]]\n","print(\"\\nExtracted Information:\", extracted_info)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J47tQqQ099v7","executionInfo":{"status":"ok","timestamp":1757564901955,"user_tz":-330,"elapsed":8748,"user":{"displayName":"Angeljyothi Cherukuri","userId":"10915435747811708744"}},"outputId":"fa181644-6e16-4607-f667-6720510a9ba9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.3980 - loss: 2.0796\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 623ms/step\n","\n","Tagged Output:\n","Elon --> NOUN\n","Musk --> NOUN\n","founded --> NOUN\n","SpaceX --> NOUN\n","in --> ADP\n","2002 --> NOUN\n","and --> CONJ\n","lives --> NOUN\n","in --> ADP\n","Texas --> NOUN\n",". --> .\n","\n","Extracted Information: ['Elon', 'Musk', 'founded', 'SpaceX', '2002', 'lives', 'Texas']\n"]}]},{"cell_type":"markdown","source":["**Aim and Algorithm**\n","\n","**Aim**\n","\n","To build a POS Tagger for unstructured text using BiLSTM in Keras and extract nouns, proper nouns, and numbers from a file.\n","\n","\n","**Algorithm**\n","\n","Import the required libraries and download NLTK datasets.\n","\n","Load the Treebank corpus, create word–tag mappings, and prepare sequences with padding.\n","\n","Build and train a BiLSTM POS tagging model using Keras.\n","\n","Read the unstructured text from the given file path and tokenize it.\n","\n","Convert tokens into sequences, predict POS tags, and extract nouns, proper nouns, and numbers.\n","\n","Display the tagged output and the extracted information."],"metadata":{"id":"MLuc94nLCfks"}}]}